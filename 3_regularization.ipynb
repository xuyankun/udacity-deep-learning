{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "Let's use assignment2 model, and add a new parameter **beta**, actually we don't what values **beta** should be, we could set it as 0.001 firstly, and then tune it!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  # we use L2 regularization\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels = tf_train_labels, logits = logits)) + beta * tf.nn.l2_loss(weights)\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-1a5f4ac297be>:4: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Minibatch loss at step 0: 22.673918\n",
      "Minibatch accuracy: 4.7%\n",
      "Validation accuracy: 12.5%\n",
      "Minibatch loss at step 500: 2.255545\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 75.7%\n",
      "Minibatch loss at step 1000: 1.590626\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 78.0%\n",
      "Minibatch loss at step 1500: 0.950144\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 79.8%\n",
      "Minibatch loss at step 2000: 0.889764\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 80.3%\n",
      "Minibatch loss at step 2500: 0.729431\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 3000: 0.794831\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.7%\n",
      "Test accuracy: 88.7%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta : 0.001}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, the test accuracy is around 88-89%, but former model without L2 regularization is around 82%. It's improved!  \n",
    "But whether or not 0.001 is the best for beta paramenter, We can look into it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-30f2a6839244>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-9-30f2a6839244>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-9-30f2a6839244>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-9-30f2a6839244>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-9-30f2a6839244>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-9-30f2a6839244>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-9-30f2a6839244>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-9-30f2a6839244>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-9-30f2a6839244>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-9-30f2a6839244>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.2)]\n",
    "accuracy_val = []\n",
    "\n",
    "for regul in regul_val:\n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta : regul}\n",
    "      _, l, predictions = session.run(\n",
    "        [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    accuracy_val.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEMCAYAAADDMN02AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl8VPW9//HXJwkhZCGBBAKEEJawC0EFARUFxX2rWhVb\na7W1au91uXbT3i7+7G2tWr29bW1tvWpb24qK6HWjQBXjRhVQFgmLJCxJ2MIWICwhy/f3xzmhY8wG\nWU5m5v18PPLIzFk/Z+Y7n/M9n3NmjjnnEBGR6BETdAAiItKxlPhFRKKMEr+ISJRR4hcRiTJK/CIi\nUUaJX0QkyijxS6dnZglm5sysf9CxHCsz+8DMrmvF/EVmNrmNY+pqZhVm1q8tlxuy/F+a2a3+4/PN\nrLANlnncMZvZfWb2aAum+62Z3Xh8EYYXJf424DfIur9aMzsU8vzLrVhuq5KGhD/n3BDn3D9bs4z6\n7cg5V+mcS3bObWl9hJ9bVxbwReCptlxuS2NuaEfjnLvXOXdbC1bzC+BeM4ttTazhQIm/DfgNMtk5\nlwwUA5eEDPtb0PG1FzOLCzqG1uqs29BZ42qBrwH/55w7EnQgx8o5txEoAS4IOJR2p8TfAcws1sx+\nZGbrzWynmf3NzNL8cUlm9qyZ7TazcjP70Mx6mNkjwATgCf/I4ZEGlhtnZrPNbLs/71tmNjxkfJKZ\n/drMSsxsr5m9XZdQzGyq3xPca2bFZvYlf/hneodmdquZveE/riu5fNPMioCV/vDHzKzUzPaZ2SIz\nm1Qvxnv9bd9nZovNrI+ZPWlmP6u3PfPN7JtNvJRfMLONZrbDzH5mnkR/uUNDltPfzA7Wvcb11nGr\nmS3wD+v3APf4w28xs7X++/C633Otm+ciM1vnv8b/E/oamdkDZvZEyLQjzKy6oeD9cfn+OnaY2Z/N\nLCVk/DYz+46ZFQD7Qoad7reh0CPLA/570cfMepnZ3/1l7jazl82srz//59qR1SudmVlPM3vGn3+D\nmX3PzCzk9XrTb0fl5pWepjfxHl0AvN3YSDMbY2bv+staYWYXhIzr7W/HPv81fqCBtlcX82VmtsbM\n9vvt+w4zSwdeAgaHvE7pDbxHDbZ9Xz5wURPbFxmcc/prwz9gIzC93rC7gXeBfkAC8Cfgj/64O4EX\ngG5AHN6HNMkf9wFwXRPrigOuB5L95T4GfBAy/klgPtAHiAWm+P9zgQrgSn8ZvYC8htYJ3Aq84T9O\nABzwOpAGdPOHXw/0ALoAP8DrNXXxx/0IWOqvMwY40Z/3DGADYP50/YCDQM8GtrNuvfP8eQcB6+vi\nxCsr3Ffv9Z7VyGt2K1ANfMN/LboB1wCrgWH+NvwUeMufvq//Wl3sj/seUBWy7geAJ0KWPwKoDnn+\nQci0I4CzgHj/PfkAeCBk2m3AYv+16BYy7PQGtuO/gTf8bcgELvO3JRV4GXi2oRjqvZ79/efPA7P8\ndpTrvy9fDnm9qvz3OBa4C9jYRJvcD4wJeX4+UBiy3mLg2/5reZ7/2g7yx/8f8LS/HWOBrXy+7dXF\nvAs4xX+cDpxYf30hMRx9j2ii7fvjvwQsDDqPtPdf4AFE2h8NJ/4NwGkhzwfhJTkD/g2vh3RCA8tq\nMvE3MH0foNb/kHTxP7DDG5juPmBmI8toSeI/tYkYzN+24f7zTcB5jUy3HpjiP/8O8GIjy6xb79SQ\nYd8CXvcfnxn6YQc+AS5tZFm3Ap/WG/ZWXaLzn9e9dpnAzfg7AX9cDFDGcST+BmKZAfwz5Pk24Ev1\npvlc4sdLwoU0sJP0x08Ctjbxnh5NokBXoAYYHDL+TmBuyOu1MmRcT3/etAbWG+uPGxgyLDTxn+O3\nBwsZ/xLeUVeC33ZzQsY93EDbq0v8ZcCNQEq9GJpL/I22fX/8JcCqln7mwvVPpZ525h8yZwNz/MPb\ncrwecAxeT+VJvMT/gl8uud9aeHLJL6M8UldGAdbgJdR0vJ5qHFDUwKzZjQxvqZJ6cXzfL5PsBfbg\nfUgz/G3PamhdzvuUPQ3UlZWuA/5yDOvdhNczBngHiDWzyWY2Dm/b/97S+IEc4Pch788OvKOC/v46\njk7vnKsFNjcTZ4PMrJ+ZzTKzzf779QSQ0Uxs9ZcxEXgEuMw5t9sflmJmT/lli314R3n1l9uYPnht\nsThk2Ca8963OtpDHB/3/yfUX5Jyrwevxp9Qf5+sHFPvvff119cFru6Uh45p6LS7D67UX+6W7CU1M\nG6q5tp8ClLdwWWFLib+d+Y18M3CWcy4t5C/BObfTeVcr/Ng5NwKv/HEVXk8QvB5OU27E60VNwzvE\nH+EPN7zD5GpgSAPzlTQyHOAAkBjyvE9Dm1X3wMzOAW4HLscrw/QEDuH16uq2vbF1PQ180cxOxvtA\nvt7IdHWyQx4PALbA53YiX8Erc1Q1sZz6r2sJcEO996ebc+4jvNfx6GWkZhbDZ5NiS16vOr/wpz/B\nOdcduAnvvWoqtqPMu5RxNnCTc64gZNQ9fowT/OWeW2+5TbWjbXg97QEhwwZwnDs3YAVeyawhW+qt\nJ3Rd2/DiDH1ts2mEc+6fzrmL8Y7K5gPP1I1qJr6m2j7ASGB5M8sIe0r8HeP3wANmlg1HT2Jd4j+e\nbmaj/ISyDy9Z1/rzbQcGN7HcFOAwXr0zCa82DYCf+J4GfmVmmf7JwdP9o4m/ABeb2eX+UUMvMxvr\nz7oMLxknmNkI4IZmti0FryyyA692/RO8Hn+dJ4D7zWyweU40/6Src249sAr4I/Cca/5KkLvNLNXM\nBgK3Ac+FjHsauBq41n98LH4P/ND8E+PmnVy/0h/3CjDRzC4078T4t/DOZ9RZBkwzsywz64F3fqEx\nKXj15X1mNsBfVouYWTzwIvAH59zLDSz3IFBuZhnAD+uNb7QdOecq8cot95t3McAQvFLPX1saWz1z\n8EpvDXkXiDGz//Db3Tl4O6nnnXOHgVeB+/y2dwJevf1z/DhnmFl3vLa3n89+Znqb2eeOSHxNtX38\n2Js6WowISvwd4yG8E3ELzGw/sBA4yR+XhXcybj/eVTJz+FdC+yVwvZntMbOHGljuk3gJdxteXfu9\neuPvwDusXYq3c/gvvJ54Id6h8n8Cu4ElwOiQWOP85T5O8wngVbxSSxFezX6nP2+dB/B68gvwdmy/\nx6sr1/kzMIbmyzz4y1nuxzsrNDbnXBGwFtjvnFvUgmUd5ZybCTwKvOiXSpbhHUnhnNuKtzP5tb9t\n/fFe68qQmF7D24F9gHeCsjE/Bk4H9uIl29nHEOZgYCLezi/06p7eeLXwDLz3+D28NhSquXZ0i/9/\nE9779ARwvJch/wnv6qv4+iP85H4x3nX+u/BOUF/jdwDq4uiH136eAGbyr9e5vq/58e7FO+dxvT98\nOd7OepNfuutZL4ZG276Z5eCV/Zo78gx7dVdUiATCzM4Ffuecy22DZT2Dd2Lup81OfPzriMPb0V7i\nWvnFqkhlZv+NdwL9961czq+ABOfcLc1O3AbM7LfAR865Nv3yWWekxC+BCSlfvOOca6gneizLygU+\nBkY65463Pt3Ysi/AO0qrxLtc9atAbgtKU3IM/PKOwzt6mozX877WOTc30MAikEo9Egj/6ps9ePXp\n37ZyWQ/hlbN+0tZJ31f3nYMy4GzgciX9dpGKVzo8gFfG+6mSfvtQj19EJMqoxy8iEmWU+EVEokyn\n/AXAjIwMN3DgwOOa98CBAyQlJbVtQCJtSG1U2sNHH3200znXqyXTdsrEP3DgQJYsWXJc8+bn5zN1\n6tS2DUikDamNSnsws00tnValHhGRKKPELyISZZT4RUSijBK/iEiUUeIXEYkySvwiIlGmU17OKSLt\np7K6hqXF5SR3jWNwryQS45UGoo3ecZEoUFFZTf7aMuYVbOetNWVUVFYfHZeV1o0hvZPJ7ZXMkN5J\n/v9k0pPi8e6eKZFGiV8kQu2sqOSNVduZv2o77xXu5Eh1LelJ8Vw0pi9nj+xNda2jqKyCwh0VFO2o\nYOaG3Ryqqjk6f1piF4b0SmZIryRyeyczpFcyub2T6d8jkdgY7RDCmRK/SAQp2X2QeQXbmF+wnSWb\ndlProH+PbnxlUg7nje7DyTk9Gk3atbWOrfsOezuDMm9nUFhWwYI1O3h+yb/ugR4fF8PgjCRvp9Db\n2zF4O4hkusXHdtSmSiso8YuEMecca7fvZ97K7cwr2MaqrfsAGNEnhdvOGsp5ozMZ1bd7i0o2MTFG\nVlo3stK6ccawz/7ky96DVUePDIr8nULBlr38feVWakN+2T0rrdtnjg7qjhZ6qmzUqSjxi4SZ2lrH\nx8V7vJ79qu1s2nUQMzhpQA/+88IRnDe6DznpbfsjcKmJXTg5pwcn5/T4zPDK6ho27jx4dIdQt3NY\n1EjZ6Oh5BH/noLJRMJT4RcLAkepaFhbtZF7Bdv6xajs7KyrpEmucOiSDW84YwvRRvemdktDhcXWN\ni2V4nxSG90n5zPC6slFhWcgOoayCN9eU8dySf90/PaFLDCfn9OC03AxOG5LBCVmp2hF0ACV+kU7q\nQGU1+Wt3MK9gG2+tKWN/ZTWJ8bFMG96bc0dnMm1Eb7ondAk6zAaFlo3OrFc2Kj94hKIdBygqq2D1\ntn38s2gXD81dC6yle0Ick4ekc3puBqfmZjA4I0klonagxC/SieyqqOSN1duZV/CvK3F6JsVz4Zi+\nnDs6k9NyM0joEt4nUNMS4zk5J/4zZaOdFZUsLNrF++t28l6hd2QD0Kd7gnc0kJvOabkZZHbv+KOa\nSKTELxKw0j0HmVfgnZxdstG7EicrrRvXTczhvNGZjB/YM+LLHxnJXbk0rx+X5vXDOUfx7oO8X7iL\n94t2smDNdmZ/7F1VlNs7mdOGeDuBSUPSO+0RT2enxC/SwZxzrN22n3kF25hXsI2CLd6VOMMzU7ht\nWi7nju7D6H4tuxInEpkZOelJ5KQn8aWJA6itdazeto/3C3fyfuEunl9Syp//uYkYgzH90zg9N53T\nhmRwUk6PsD8a6ijmnGt+qg42fvx4pztwSaSo68EuKylnaXE5c5ZtouygO3olzrmjMjlvdB8GZuh2\njC1xpLqWpcV7eL9oF+8X7mRZSTk1tY6ucTFMGNiTU3O9cwSj+0XXiWIz+8g5N74l06rHL9LGdlZU\nsryknOUl5Swr3cuK0nLKD1YB3lUsuakx3HneSM4ZmUlv1ayPWXxcDBMHpzNxcDrfOmcY+w9XsWjD\nbt4v3MXCop08NHctD7GW1G5dmDw4/ej5gUE6UXyUEr9IKxyorGbl5r0sLy1neclelpWUs7n8EAAx\nBsMyUzh/dB/ystPI65/GsMxk3nv3HaZOzAk48siRktCFs0dmcvbITAB27K9kYdHOo6WhuQXbAOib\nmsCpQzI4fahXGormna4Sv0gLVdXU8un2/Swv2ev16EvL+XT7/qPfXO3foxvjBqRxw6kDyctO44Ss\n7vrlywD0SunKZeOyuGxcFs45Nu06yPtFO1lYuIs3650oPj03g1OHpEfdiWK1SpEGhNbll5d45ZqV\nW/ZyuKoWgB6JXcjLTuO80X3Iy05lbP80MpK7Bhy11GdmDMxIYmBGEl+emENtrWPVVv9EcdEunl1c\nzJ8WbiQuxpgyNINL8vpxzqhMUiJ8J6DEL4JXl19RWs6ykN58XV2+a1wMY7JS+fLEHPKy0xjXP43s\nnt1ULw5DMTHGCVmpnJCVyi1nDjl6b4IFa8p4fcVWvvX8cuLjYpg2vBeX5PXjrBG9I/KoLfK2SKQZ\nB49Us3LzPv/kq3cStnTPZ+vy543y6/LZqQzLTKFLrG5WF4m6xsUyaXA6kwanc8/5I1haUs6ry7cw\n55OtzCvYTrcusUwflcklY/ty5vBedI2LjMtFlfglKqzcvJe/frCJZSWfr8vnZafx1cmqy0e7mBg7\n+kN0P7p4FIs27ObVFVv4+ydbeXX5FlK6xnHu6D5ckteX03IzwrozoBYuEa+qppZb/vIR+w5VcVJO\nD84d3YdxqstLE2JjjMlD0pk8JJ37Lh3NwqJdvLZ8C3MLtjH741J6JHbh/BP6ckleXyYOSg+77wso\n8UvEe+njzWwuP8Qfb5jAtBG9gw5HwkyX2BjOHNaLM4f14qeXn8A7n+7ktRVbeHnZZmYuKqZXSlcu\nGtOXi8f25aQBPYgJg52AEr9EtOqaWn6XX8gJWd2ZOrxX8zOINKFrXCznjMrknFGZHDpSw4I1Zby2\nYgszF3lXB/VLTeDivH5cPLYvY7JSO+0FAEr8EtFeW7GVjbsO8vvrTu60H0IJT93iY7lobF8uGtuX\nispq3li1nVeXb+GP72/g8XfWk5OeyCVj+3FxXl+GZ6Z0qvanxC8Rq7bW8ehbhQzPTOHcUZlBhyMR\nLLlrHF84MYsvnJjF3oNVzCvYxqsrtvDY20U8+lYhQ3snc/HYflyS15fBvZKDDleJXyLX3IJtFJZV\n8OtrTwyLuqtEhtTELlw9IZurJ2Szs6KSv6/cxqvLt/A/b37KL9/4lNH9unPxWK8clN0zMZAYlfgl\nIjnn+M2CQgb3SuKiMX2DDkeiVEZyV74yKYevTMph297DvO5fGvrg3DU8OHcN47LTuCSvHxeN6Uuf\n1I777SAlfolIb64uY/XWfTxyVV7YXWonkalPagJfP30QXz99ECW7D/Laiq28tmIL//XaKn76+iom\nDOzJJXn9mDEhu92/I9CipZvZXWZWYGYrzWymmSWY2Tgz+8DMlpnZEjM7pZF5N5rZJ3XTtW34Ip/n\n9fbXkd2zG5eO6xd0OCKfk90zkW9OHcLrd0zhzW+fyX+cPYzdB47w+DtFxHVAR6XZHr+ZZQF3AKOc\nc4fM7HlgBvAl4D7n3N/N7ELgIWBqI4uZ5pzb2UYxizTpnXU7WV66l59fMSasv10p0WFIr2TunD6U\nO87OZc/Bqg65+qeln4o4oJuZxQGJwBbAAd398an+MJFAOef4zZvr6JuawJUn9Q86HJEWMzN6JsV3\nyLqa7fE75zab2cNAMXAImO+cm29mJcA8f1wMcGpjiwDeMLMa4A/OuccbmsjMbgZuBsjMzCQ/P/+Y\nNwagoqLiuOeV8Ld6Vw1LNh3mupHxLHzvnaDDaZDaqASt2XvumlkPYDZwDVAOzAJeAE4B3nbOzTaz\nq4GbnXPTG5g/y9959Ab+AdzunGvyE6l77srx+tL/fsC6sgre/d60TnvjbbVRaQ/Hcs/dlpR6pgMb\nnHM7nHNVwIt4vfuv+o/B2xk0eHLXObfZ/18GvNTYdCKt9dGm3Sws2sUtZwzutElfpDNoSeIvBiaZ\nWaJ5Zx3OBlbj1fTP9Kc5C1hXf0YzSzKzlLrHwLnAyrYIXKS+3ywopGdSPF+aOCDoUEQ6tZbU+D80\nsxeAj4FqYCnwuP//V/4J38P49Xkz6wc84Zy7EMgEXvLPUscBzzjn5rbHhkh0W1FaTv7aHXz3vOH6\nPX2RZrToE+Kcuxe4t97g94CTG5h2C3Ch/3g9kNfKGEWa9eiCQronxHH95JygQxHp9HSRs4S91Vv3\nMX/Vdm48bVDE3yRbpC0o8UvY++1bhSR3jePG0wYGHYpIWFDil7BWWFbB659s5SuTc0hL7Jgvv4iE\nOyV+CWu/e6uQhLhYbjp9UNChiIQNJX4JW5t2HeDl5Vv48sQBpOum6SItpsQvYeux/CJiY4ybzxgc\ndCgiYUWJX8LS5vJDzP64lBkTsundveNuYCESCZT4JSz94e0iAG45c0jAkYiEHyV+CTtl+w7z7OIS\nrjypP1lp3YIORyTsKPFL2PnDO+upqXX829TcoEMRCUtK/BJWdlVU8rcPN3HZuH4MSE8MOhyRsKTE\nL2Hlifc2UFldq96+SCso8UvYKD94hKcXbuSiMX3J7Z0cdDgiYUuJX8LGH9/fyIEjNdx2lnr7Iq2h\nxC9hYf/hKv74/gbOHZXJiD7dgw5HJKwp8UtYePqfm9h3uJrbzxoadCgiYU+JXzq9g0eqefK9DUwb\n3osx/VODDkck7CnxS6f3tw+K2X3gCLepty/SJpT4pVM7XFXD4++u57TcdE7O6RF0OCIRQYlfOrXn\nFpewY38lt01Tb1+krSjxS6dVWV3D798uYsLAHkwa3DPocEQihhK/dFovfryZrXsPc/tZQzGzoMMR\niRhK/NIpVdXU8rv8QvL6pzJlaEbQ4YhEFCV+6ZReXraFkt2H1NsXaQdK/NLp1NQ6fvdWISP7dufs\nkb2DDkck4ijxS6fz+idbWb/zALeflavevkg7UOKXTqW21vHognUM7Z3M+aP7BB2OSERS4pdOZf6q\n7Xy6vYLbzsolJka9fZH2oMQvnYZzjt8sWMfA9EQuGtM36HBEIpYSv3Qa+Wt3ULBlH/82LZe4WDVN\nkfaiT5d0Cs45fr1gHVlp3bj8xKygwxGJaEr80im8X7iLpcXlfHPqELqoty/SrvQJk07hNwvW0ad7\nAleN7x90KCIRT4lfArdow24+3LCbW84cTNe42KDDEYl4SvwSuN8sWEdGcjwzJgwIOhSRqNCixG9m\nd5lZgZmtNLOZZpZgZuPM7AMzW2ZmS8zslEbmPd/M1ppZoZnd07bhS7hbWryHd9ft5BtTBtMtXr19\nkY7QbOI3syzgDmC8c+4EIBaYATwE3OecGwf82H9ef95Y4LfABcAo4FozG9V24Uu4e3RBIWmJXbhu\nUk7QoYhEjZaWeuKAbmYWByQCWwAHdPfHp/rD6jsFKHTOrXfOHQGeBS5rXcgSKVZu3suba8r4+mmD\nSOoaF3Q4IlGj2U+bc26zmT0MFAOHgPnOuflmVgLM88fFAKc2MHsWUBLyvBSY2NB6zOxm4GaAzMxM\n8vPzj2U7jqqoqDjueaVjPbr0MN3iYEhtKfn5m4MOp8OojUrQmk38ZtYDr5c+CCgHZpnZdXi9+buc\nc7PN7GrgSWD68QbinHsceBxg/PjxburUqce1nPz8fI53Xuk4n27fz5K573D7WblceM7woMPpUGqj\nErSWlHqmAxucczucc1XAi3i9+6/6jwFm4e0I6tsMZIc87+8Pkyj36IJCkuJj+dppg4IORSTqtCTx\nFwOTzCzRvB9HPxtYjVfTP9Of5ixgXQPzLgaGmtkgM4vHOyn8SuvDlnC2fkcFr63YwnWTc+iRFB90\nOCJRpyU1/g/N7AXgY6AaWIpXklkK/Mo/4XsYvz5vZv2AJ5xzFzrnqs3sNmAe3tVATznnCtpnUyRc\n/C6/iPi4GG46fXDQoYhEpRZdSuGcuxe4t97g94CTG5h2C3BhyPM5wJxWxCgRpGT3QV5aupnrJ+fQ\nK6Vr0OGIRCV9c1c61GNvFxFrxi1nDAk6FJGopcQvHWbr3kO8sKSUq8b3p09qQtDhiEQtJX7pMH94\nez21zvHNqertiwRJiV86RNn+w8xcVMwVJ2XRv0di0OGIRDUlfukQT7y7gaqaWv5tam7QoYhEPSV+\naXe7Dxzhrx9s4tK8fgzMSAo6HJGop8Qv7e6p9zZwqKqGf5+m3r5IZ6DEL+1q76Eq/rxwIxec0Ieh\nmSlBhyMiKPFLO/vtW4Xsr6zmtmlDgw5FRHxK/NJuFhbu5H/fXc+1pwxgVL/uzc8gIh1CiV/axZ4D\nR7jr+WUMzkjiRxePDDocEQmh2x5Jm3POcc+LK9h94AhPfnUCifFqZiKdiXr80uZmLiphXsF27j5/\nBCdkpQYdjojUo8QvbaqwrIKfvFbAlKEZusmKSCelxC9tprK6hjtmLiUxPo5HrsojJsaCDklEGqDi\nq7SZh+etZdXWfTxx/Xh6d9evb4p0VurxS5t4d90O/vfdDXxlUg7TR2UGHY6INEGJX1ptV0Ul33p+\nOUN7J/ODi3Tppkhnp1KPtIpzjrtnr2DvoSqe/topJHSJDTokEWmGevzSKn/9YBNvrC7j+xeMYGRf\nfTtXJBwo8ctx+3T7fn76+mqmDu/FDacODDocEWkhJX45LoervEs3UxLi+MUX8zDTpZsi4UI1fjku\nD85dw5pt+/njjRPoldI16HBE5Bioxy/H7K21Zfzx/Y3ceNpApg3vHXQ4InKMlPjlmOzYX8l3Zy1n\nRJ8U7j5/RNDhiMhxUKlHWsw5x3dfWM7+w9U8841JunRTJEypxy8t9qeFG8lfu4MfXjSSYbqNokjY\nUuKXFlm9dR8/n7OG6SN7c92knKDDEZFWUOKXZtVdupma2IUHrxyrSzdFwpxq/NKs++esZl1ZBX/5\n+imkJ+vSTZFwpx6/NOmNVdt5+p+b+MaUQUwZ2ivocESkDSjxS6PK9h3me7NXMKpvd75z3vCgwxGR\nNqLELw2qrXV8e9ZyDh6p5tfXnkjXOF26KRIplPilQU+9v4F31+3kxxePJrd3ctDhiEgbUuKXz1m5\neS8Pzl3DeaMzufaU7KDDEZE2psQvn3HwSDV3PruUnknxPHCFLt0UiUQtupzTzO4CbgIc8AlwI/Bn\noO6MXxpQ7pwb18C8G4H9QA1Q7Zwb3/qwpb3812urWb/zAH/7+kR6JMUHHY6ItINmE7+ZZQF3AKOc\nc4fM7HlghnPumpBpHgH2NrGYac65na2OVtrV3JXbmLmomFvPHMKpuRlBhyMi7aSlX+CKA7qZWRWQ\nCGypG2FeLeBq4Ky2D086yra9h7nnxRWMyUrlW+cMCzocEWlHzSZ+59xmM3sYKAYOAfOdc/NDJpkC\nbHfOrWtsEcAbZlYD/ME593hDE5nZzcDNAJmZmeTn57d8K0JUVFQc97zRqtY5frH4MIcqa/ny4CMs\nfO+doEOKaGqjErSWlHp6AJcBg4ByYJaZXeec+6s/ybXAzCYWcbq/8+gN/MPM1jjnPpdZ/B3C4wDj\nx493U6dOPbYt8eXn53O880arx/KLWL17DQ9dOZarJ+gqnvamNipBa8lVPdOBDc65Hc65KuBF4FQA\nM4sDrgCea2xm59xm/38Z8BJwSmuDlrazorScR+av5aIxfblqfP+gwxGRDtCSxF8MTDKzRL+efzaw\n2h83HVjjnCttaEYzSzKzlLrHwLnAytaHLW3hQGU1dz67jN4pXbn/8jG6dFMkSjSb+J1zHwIvAB/j\nXcoZg1+SAWZQr8xjZv3MbI7/NBN4z8yWA4uA151zc9sodmml+14tYOOuA/zymnGkJnYJOhwR6SAt\nuqrHOXeuTeAnAAANRUlEQVQvcG8Dw29oYNgW4EL/8Xogr3UhSnt4fcVWnl9Sym3Tcpk4OD3ocESk\nA+mbu1Foc/khvv/iCsZlp3Hn9KFBhyMiHUyJP8rU1Druem4ZNbWOX80YR5dYNQGRaKM7cEWZx/IL\nWbRhN49clUdOelLQ4YhIANTdiyJLi/fwyzfWcWleP644KSvocEQkIEr8UWL/4SrufHYZfbon8NPL\nT9ClmyJRTKWeKHHvKwWU7jnI87dMpnuCLt0UiWbq8UeBl5dt5sWPN3P7WUMZP7Bn0OGISMCU+CNc\nye6D/PCllZyc04Pbz8oNOhwR6QSU+CNYdU0t//HcMgD+55pxxOnSTRFBNf6I9uhbhXy0aQ+/mjGO\n7J6JQYcjIp2EuoAR6qNNe/j1m+u44sQsLhunSzdF5F+U+CPQoSM1fGfWcvqmduO+y0YHHY6IdDIq\n9USgB+euYcPOAzzzjYmk6NJNEalHPf4Is7BoJ39auJEbTh3IqUN0w3QR+Twl/giy/3AV3521gkEZ\nSdx9/oigwxGRTkqlnghy/5zVbN17iFm3nkq3+NigwxGRTko9/gjx1toyZi4q4eYzhnByTo+gwxGR\nTkyJPwLsPVjFPbNXMCwzmbvO0Y1VRKRpKvVEgP/3agG7Ko7w5Fcn0DVOJR4RaZp6/GFu7sptvLR0\nM7edlcsJWalBhyMiYUCJP4ztqqjkBy99wglZ3fn3afoBNhFpGZV6wpRzjh/+30r2H67mmat071wR\naTllizD1yvIt/H3lNr517jCG90kJOhwRCSNK/GFo+77D/PjlAk4akMY3pgwOOhwRCTNK/GHGOcc9\ns1dQWV3Dw1flERuje+eKyLFR4g8zs5aU8tbaHdxz/ggG90oOOhwRCUNK/GGkdM9BfvLaKiYPTuf6\nyQODDkdEwpQSf5iorXV874UVOOd46ItjiVGJR0SOkxJ/mPjrh5tYWLSLH148SrdRFJFWUeIPAxt3\nHuDnc9Zw5rBezJiQHXQ4IhLmlPg7uZpax3dmLadLrPHglWMxU4lHRFpH39zt5J58bz1LNu3hl9fk\n0Sc1IehwRCQCqMffia3bvp+H53/KeaMz+cK4rKDDEZEIocTfSVXV1PLtWctJ7hrHzy4foxKPiLQZ\nlXo6qcfyi1hRupfHvnwSGcldgw5HRCJIi3r8ZnaXmRWY2Uozm2lmCWb2nJkt8/82mtmyRuY938zW\nmlmhmd3TtuFHpoIte/n1m+u4bFw/LhjTN+hwRCTCNNvjN7Ms4A5glHPukJk9D8xwzl0TMs0jwN4G\n5o0FfgucA5QCi83sFefcqrbagEhTWV3Dt59fTs+keO67dHTQ4YhIBGppjT8O6GZmcUAisKVuhHnF\n56uBmQ3MdwpQ6Jxb75w7AjwLXNa6kCPbr95Yx5pt+3ngyjGkJcYHHY6IRKBme/zOuc1m9jBQDBwC\n5jvn5odMMgXY7pxb18DsWUBJyPNSYGJD6zGzm4GbATIzM8nPz2/RBtRXUVFx3PMGrai8hsc+OMwZ\n/eOI2baa/G2rgw5J2kE4t1GJDC0p9fTA66UPAsqBWWZ2nXPur/4k19Jwb/+YOOceBx4HGD9+vJs6\ndepxLSc/P5/jnTdIh6tq+Mmv36VfWjd+e9MUUhK6BB2StJNwbaMSOVpS6pkObHDO7XDOVQEvAqcC\n+KWfK4DnGpl3MxD6GwP9/WFSzy/mrWX9jgM89MWxSvoi0q5akviLgUlmlujX888G6moQ04E1zrnS\nRuZdDAw1s0FmFg/MAF5pbdCR5oP1u3jq/Q1cPzmH03Izgg5HRCJcs4nfOfch8ALwMfCJP8/j/ugZ\n1CvzmFk/M5vjz1sN3AbMw9tZPO+cK2iz6CPAgcpqvvvCcgb0TOSeC0YEHY6IRIEWfYHLOXcvcG8D\nw29oYNgW4MKQ53OAOccfYmS7f85qSvccYtYtk0mM1/fpRKT96ScbAvT2pzv424fFfGPKYMYP7Bl0\nOCISJZT4A7L3UBV3v7CC3N7JfOucYUGHIyJRRIk/ID95dRU7Kir576vzSOgSG3Q4IhJFlPgD8I9V\n25n9cSn/PnUIY/unBR2OiEQZJf4OtvvAEb7/4ieM6tud284aGnQ4IhKFdBlJB/vRyyvZe+gIf73p\nFOLjtN8VkY6nzNOBXl2+hddXbOU/pg9jRJ/uQYcjIlFKib+DlO0/zI9eXsm47DRuOWNw0OGISBRT\n4u8Azjn+88VPOHSkhkeuziMuVi+7iARHGagDzP54M2+sLuN7549gSK/koMMRkSinxN/OtpQf4r5X\nCpg4qCc3njow6HBERJT425Nzjrtnr6DGOX7xxTxiYizokERElPjb098+LObddTv5wUUjGZCeGHQ4\nIiKAEn+72bTrAPfPWc2UoRl86ZQBQYcjInKUEn87qK11fHfWCmJjjAevHIt3/xoRkc5B39xtB0+9\nv4FFG3fzyFV59EvrFnQ4IiKfocTfRpxzLCsp59lFJby0dDPTR2ZyxUlZQYclIvI5SvyttPdgFS8t\nLeXZxSWs2bafxPhYLj8xi3suGKESj4h0Skr8x8E5x6INu3l2cQlzPtlKZXUtY/uncv/lY7gkry8p\nCV2CDlFEpFFK/MdgV0UlL368mZmLi1m/4wApXeO4enw2M07JZnS/1KDDExFpESX+ZtTWOhYW7WLm\n4mLmF2yjqsZxck4PfvHFIVw0tq9ukC4iYUdZqxFl+w4z66NSnltcQvHug6QlduErkwYy45RshmWm\nBB2eiMhxU+IPUVPrePvTMmYuKmHBmjJqah2TB6fz7XOHcd7oPro3rohEBCV+YHP5IZ5bXMKsJSVs\n3XuYjOR4bpoyiBkTBjAoIyno8ERE2lTUJv6qmlreXF3Gs4uLefvTHQBMGdqLH188irNHZuq2iCIS\nsaIu8W/adYBnF5fwwkel7NhfSZ/uCdw+LZerxmeT3VM/pCYikS8qEn9ldQ3zC7Yzc1ExC4t2ERtj\nTBvem2tPyebMYb10RywRiSoRnfgLy/bz7KISZn9cyp6DVfTv0Y1vnzOMq8Zn0yc1IejwREQCEXGJ\nv7LGMfujUp5dXMzijXuIizHOHZ3JjAkDOD03QzdDEZGoFzGJ/0BlNQ/OXcOsxQc5VL2cQRlJfP+C\nEVx5cn8ykrsGHZ6ISKcRMYm/W5dYPli/i7xesdx58QQmDe6pH0kTEWlAxJzVjIkx5twxhVvzEpg8\nJF1JX0SkERGT+AFdnSMi0gLKlCIiUUaJX0Qkyijxi4hEmRYlfjO7y8wKzGylmc00swR/+O1mtsYf\n91Aj8240s0/MbJmZLWnL4EVE5Ng1ezmnmWUBdwCjnHOHzOx5YIaZbQIuA/Kcc5Vm1ruJxUxzzu1s\nm5BFRKQ1WlrqiQO6mVkckAhsAb4JPOCcqwRwzpW1T4giItKWmu3xO+c2m9nDQDFwCJjvnJvvl3am\nmNnPgMPAd5xzixtaBPCGmdUAf3DOPd7QeszsZuBmgMzMTPLz849rgyoqKo57XpGOoDYqQTPnXNMT\nmPUAZgPXAOXALOAF4B7gLbwy0ATgOWCwq7dAM8vydx69gX8Atzvn3mlmnTuATY2MTgX2NjF7BhAp\nZaXmtjWc1tna5R7P/Mc6T0umb4tp1EY753rDvY3mOOd6tWgpzrkm/4CrgCdDnl8P/A6Yi1e7rxte\nBPRqZln/D+/IoNn1NrGMx5sZv6Q1y+9Mf81tazits7XLPZ75j3WelkzfFtOojXbO9UZKG23JX0tq\n/MXAJDNLNO93EM4GVgP/B0wDMLNhQDz1ejFmlmRmKXWPgXOBlS1YZ1NebeX84SSIbW2vdbZ2uccz\n/7HO05Lp22qaSBHUtrbHeiOljTar2VIPgJndh1fqqQaWAjfh1e6fAsYBR/B68gvMrB/whHPuQjMb\nDLzkLyYOeMY597O2CLyJWJc458a35zpEWkNtVILWosQfTszsZtfICWSRzkBtVIIWcYlfRESapp9s\nEBGJMkr8IiJRRolfRCTKRFXi9y8vXWJmFwcdi0hDzGykmf3ezF4ws28GHY9EprBI/Gb2lJmVmdnK\nesPPN7O1ZlZoZve0YFF3A8+3T5QS7dqinTrnVjvnbgWuBk5rz3gleoXFVT1mdgZQATztnDvBHxYL\nfAqcA5QCi4FrgVjg5/UW8TUgD0gHEoCdzrnXOiZ6iRZt0U6dc2VmdinejyD+xTn3TEfFL9Gj2R9p\n6wycc++Y2cB6g08BCp1z6wHM7FngMufcz4HPlXLMbCqQBIwCDpnZHOdcbXvGLdGlLdqpv5xXgFfM\n7HVAiV/aXFgk/kZkASUhz0uBiY1N7Jz7AYCZ3YDX41fSl45wTO3U76BcAXQF5rRrZBK1wjnxHxfn\n3J+CjkGkMc65fCA/4DAkwoXFyd1GbAayQ57394eJdCZqp9LphHPiXwwMNbNBZhYPzABeCTgmkfrU\nTqXTCYvEb2YzgX8Cw82s1My+7pyrBm4D5uH9TPTzzrmCIOOU6KZ2KuEiLC7nFBGRthMWPX4REWk7\nSvwiIlFGiV9EJMoo8YuIRBklfhGRKKPELyISZZT4RUSijBK/iEiUUeIXEYky/x86PzZ2p1O6twAA\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x166c47636a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.semilogx(regul_val, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see 0.001 is not the best from the figure, but is still good enough.  \n",
    "Then we use 1-hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  logits = tf.matmul(lay1_train, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels)) + \\\n",
    "      beta * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-14-221e63ce11ec>:4: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Minibatch loss at step 0: 639.475525\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 28.2%\n",
      "Minibatch loss at step 500: 199.801575\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 80.3%\n",
      "Minibatch loss at step 1000: 113.933022\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 81.7%\n",
      "Minibatch loss at step 1500: 68.195717\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 83.8%\n",
      "Minibatch loss at step 2000: 41.186638\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 85.2%\n",
      "Minibatch loss at step 2500: 25.142754\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at step 3000: 15.482091\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 86.8%\n",
      "Test accuracy: 93.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta : 0.001}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we change the batch_size from 128 to lower values, like 10, 20..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  logits = tf.matmul(lay1_train, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels)) + \\\n",
    "      beta * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-17-ac898dce4b64>:4: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Minibatch loss at step 0: 818.895813\n",
      "Minibatch accuracy: 10.0%\n",
      "Validation accuracy: 19.8%\n",
      "Minibatch loss at step 500: 6295.328125\n",
      "Minibatch accuracy: 70.0%\n",
      "Validation accuracy: 57.0%\n",
      "Minibatch loss at step 1000: 63016.699219\n",
      "Minibatch accuracy: 60.0%\n",
      "Validation accuracy: 55.4%\n",
      "Minibatch loss at step 1500: 26256.664062\n",
      "Minibatch accuracy: 60.0%\n",
      "Validation accuracy: 47.1%\n",
      "Minibatch loss at step 2000: 367629.906250\n",
      "Minibatch accuracy: 30.0%\n",
      "Validation accuracy: 46.6%\n",
      "Minibatch loss at step 2500: 16305.705078\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 40.2%\n",
      "Minibatch loss at step 3000: 9136.805664\n",
      "Minibatch accuracy: 30.0%\n",
      "Validation accuracy: 45.9%\n",
      "Test accuracy: 50.4%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    #offset = step % num_bacthes\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta : 0.001}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apperantly, lower **batch_size** causes large loss and lower Test accuracy, if batch_size is less 5, the loss even cannot converge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta = tf.placeholder(tf.float32)\n",
    "  dropout_prob = tf.placeholder(tf.float32) # add dropout probility\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    \n",
    "  drop = tf.nn.dropout(lay1_train, dropout_prob) \n",
    "\n",
    "  logits = tf.matmul(lay1_train, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels)) + \\\n",
    "      beta * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-20-30459d491062>:4: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Minibatch loss at step 0: 663.864380\n",
      "Minibatch accuracy: 10.0%\n",
      "Validation accuracy: 18.6%\n",
      "Minibatch loss at step 500: 64716.601562\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 53.9%\n",
      "Minibatch loss at step 1000: 526681.875000\n",
      "Minibatch accuracy: 70.0%\n",
      "Validation accuracy: 54.3%\n",
      "Minibatch loss at step 1500: 871666.375000\n",
      "Minibatch accuracy: 70.0%\n",
      "Validation accuracy: 41.2%\n",
      "Minibatch loss at step 2000: 18626412.000000\n",
      "Minibatch accuracy: 40.0%\n",
      "Validation accuracy: 34.3%\n",
      "Minibatch loss at step 2500: 1292116.125000\n",
      "Minibatch accuracy: 60.0%\n",
      "Validation accuracy: 43.6%\n",
      "Minibatch loss at step 3000: 424907.968750\n",
      "Minibatch accuracy: 30.0%\n",
      "Validation accuracy: 52.0%\n",
      "Test accuracy: 56.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta : 0.001, dropout_prob : 0.5}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we set **batch_size = 10** which is small enough to converge hardly, we could use dropout with 0.5 probility to improve about 5% accuracy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 512\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta = tf.placeholder(tf.float32)\n",
    "  dropout_prob = tf.placeholder(tf.float32)\n",
    "  global_step = tf.Variable(0)\n",
    " \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal(\n",
    "        [image_size * image_size, num_hidden_nodes1],\n",
    "        stddev=np.sqrt(2.0 / (image_size * image_size)))\n",
    "    )\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "  biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "  weights3 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes2, num_labels], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "  biases3 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  drop1 = tf.nn.dropout(lay1_train, dropout_prob)\n",
    "  lay2_train = tf.nn.relu(tf.matmul(drop1, weights2) + biases2)\n",
    "  drop2 = tf.nn.dropout(lay1_train, dropout_prob)\n",
    "  logits = tf.matmul(lay2_train, weights3) + biases3\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels)) + \\\n",
    "      beta * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2) + tf.nn.l2_loss(weights3))\n",
    "  \n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 500, 0.9, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay2_test, weights3) + biases3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-b0629e5316d5>:4: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Minibatch loss at step 0: 4.022943\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 28.3%\n",
      "Minibatch loss at step 1000: 0.918353\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 2000: 0.574508\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 3000: 0.585209\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 4000: 0.665304\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 5000: 0.494454\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 6000: 0.598259\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 7000: 0.542471\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 8000: 0.547562\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 9000: 0.551219\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 10000: 0.462978\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.5%\n",
      "Test accuracy: 95.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta : 0.0013, dropout_prob : 0.6}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 1000 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
